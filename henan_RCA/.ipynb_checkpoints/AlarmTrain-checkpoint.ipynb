{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "warnings.filterwarnings('ignore')           #不显示警告\n",
    "print(\"导入包完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1、读取数据\n",
    "Dir = '/root/AI告警分析数据/告警'\n",
    "File = os.path.join(Dir, 'alarm_20190914.csv')\n",
    "df = pd.read_csv(File, sep='^')\n",
    "print(\"完成数据加载！\")\n",
    "print(\"原始数据维度\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2、查看字段名称和数据\n",
    "df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3、过滤4级告警\n",
    "df = df[df['网管告警级别']!=4].copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"滤除4级告警后的数据维度为\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#告警标题缺失数为69\n",
    "df['告警标题'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4、过滤衍生告警 和未匹配告警\n",
    "df = df[df['告警标题'].apply(lambda x:np.nan if '衍生' in str(x) else x).notnull()].copy()\n",
    "df = df[df['告警标题']!='未匹配告警'].copy()\n",
    "print(\"滤除衍生告警后的数据维度为\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4、加载补充信息表，如厂家ID与厂家名的映射关系，厂家名与厂家告警ID的映射关系，网元名称与网元ID的映射关系\n",
    "\n",
    "#厂家ID与厂家名的映射关系\n",
    "vendor_ring = pd.read_csv('/root/AI告警分析数据/ring_vendor_id_name.csv',sep='\\t')\n",
    "vendor_ring.rename(columns={'device_id':'厂家ID'}, inplace=True)\n",
    "vendor_ring.drop_duplicates('厂家ID', inplace=True)\n",
    "vendor_ring['厂家ID'] = vendor_ring['厂家ID'].apply(lambda x:np.nan if '-' in x else int(x))\n",
    "\n",
    "#厂家ID与厂家名的映射关系\n",
    "vendor_nring = pd.read_csv('/root/AI告警分析数据/notring_vendoer_id_name.csv',sep='\\t')\n",
    "vendor_nring.rename(columns={'device_id':'厂家ID'}, inplace=True)\n",
    "vendor_nring.drop_duplicates('厂家ID', inplace=True)\n",
    "\n",
    "#网元名称与网元ID的映射关系\n",
    "device_id_name = pd.read_csv('/root/AI告警分析数据/device_id_type.csv',sep='\\t')\n",
    "\n",
    "#厂家名与厂家告警ID的映射关系\n",
    "version = pd.read_csv('/root/codes/RCA/alarm_norm_file/version.csv')\n",
    "version.rename(columns={'网管告警ID':'alarmid_sup'}, inplace=True)\n",
    "version.dropna(subset=['厂家'], inplace=True)\n",
    "version.drop_duplicates(subset=['厂家', '厂家告警ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5、拼接设备类型名称\n",
    "print(\"网管告警ID缺失数据条数\",df['网管告警ID'].isnull().sum())\n",
    "data = df.merge(device_id_name, on=['设备类型ID'], how='left')\n",
    "\n",
    "#切分动环和非动环数据\n",
    "df_nr = data[data['专业ID']!=8].copy()\n",
    "df_r = data[data['专业ID']==8].copy()\n",
    "\n",
    "#拼接厂家设备名\n",
    "df_nr = df_nr.merge(vendor_nring, on=['厂家ID'], how='left')\n",
    "df_r = df_r.merge(vendor_ring, on=['厂家ID'], how='left')\n",
    "data = pd.concat([df_nr, df_r], axis=0)\n",
    "data.rename(columns={'device_name':'厂家'}, inplace=True)\n",
    "data['厂家'] = data['厂家'].apply(lambda x:'摩托罗拉' if '摩托罗拉' in str(x) else x)\n",
    "print(data.shape)\n",
    "\n",
    "#拼接厂家告警ID,只填充少数网管告警ID\n",
    "data  = data.merge(version[['厂家', '厂家告警ID', 'alarmid_sup']], on=['厂家', '厂家告警ID'], how='left')\n",
    "data.loc[data['网管告警ID'].isnull(), '网管告警ID'] = data[data['网管告警ID'].isnull()]['alarmid_sup']\n",
    "print(\"网管告警ID仍然缺失数据条数\",data['网管告警ID'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6、转换标准时间为时间戳，用于密度聚类\n",
    "data['timestamp'] = pd.to_datetime(data['发生时间'])\n",
    "data['timestamp'] = pd.to_timedelta(data.timestamp).dt.total_seconds()\n",
    "data.sort_values(by=['timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7、选取训练数据字段\n",
    "train = data[['专业ID', '告警标题', '厂家告警级别',  '网元名称', '厂家', '设备类型名称', '地市ID', 'timestamp']]\n",
    "train.columns = ['specialtyid', 'alarm_title', 'alarm_level', 'deveice_name', 'vendor', 'device_type_name', 'city', 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8、拼接专业类型名称\n",
    "specialty_dict = {1:'无线',2:'核心网',6:'传输',8:'动环'}\n",
    "train['specialty'] = train['specialtyid'].map(specialty_dict)\n",
    "train.groupby(['specialty']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成告警标题-网元名的对\n",
    "train['pair_id'] = train['alarm_title'] + '_' + train['deveice_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9、逐地市对告警信息进行根因分析，首先对不同地市采取聚类，基本划分为5-10分钟为一段周期，根据先前模型训练得到的规则表，对其进行告警压缩\n",
    "rule = []\n",
    "city_eps_ms = {\n",
    " 2000:(5,1), 2001:(11,1), 2002:(6,1), 2003:(11,1), 2004:(7,1), 2005:(5,1), 2006:(14,1),\n",
    " 2007:(11,1), 2008:(5,1), 2009:(11,1), 2010:(11,1), 2011:(3,1), 2012:(10,1), 2013:(8,1)\n",
    "}\n",
    "for city, (eps, ms) in city_eps_ms.items():\n",
    "    #9.1、采用DBSCAN聚类划分时域\n",
    "    group = train.query('地市ID==%d'%city).copy()\n",
    "    info_table = group.drop_duplicates(subset=['pair_id'], keep='first')\n",
    "    features = group[['timestamp']].values\n",
    "    clustering = DBSCAN(eps=eps, min_samples=ms).fit(features)#labal=226\n",
    "    labels = clustering.labels_\n",
    "    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    print('Estimated number of clusters: %d' % num_clusters)\n",
    "    print('Estimated number of noise points: %d' % n_noise)\n",
    "    print(\"完成聚类！\")\n",
    "    group['label'] = labels\n",
    "\n",
    "    #9.2、滤除异常点\n",
    "    group = group[group['label'] != -1]\n",
    "    group.reset_index(drop=True, inplace=True)\n",
    "    print(\"训练数据维度\", group.shape)\n",
    "\n",
    "    st = time.time()\n",
    "\n",
    "    itemsets = []\n",
    "    SupL1, TotalList = {}, []\n",
    "    lift, conf = 1, 0.7\n",
    "    for k, group_ in group.groupby('label'):\n",
    "        TotalList.extend(list(group_['pair_id'].values))\n",
    "        items = frozenset(group_['pair_id'].values)\n",
    "        itemsets.append(items)\n",
    "        for item in items:\n",
    "            SupL1[item] = SupL1.get(item, 0)\n",
    "            SupL1[item]+=1\n",
    "    SupL1 = {item:freq for item, freq in SupL1.items() if freq>=support}\n",
    "    print('频繁项数', len(SupL1))\n",
    "\n",
    "    ds_dict = dict(zip(itemsets, range(len(itemsets))))\n",
    "\n",
    "    #生成Eclat算法的输入\n",
    "    ec_data = {}\n",
    "    for trans, idxmap in ds_dict.items():\n",
    "        for item in SupL1:\n",
    "            if item in trans:\n",
    "                if item in ec_data:\n",
    "                    ec_data[item].append(idxmap)\n",
    "                else:\n",
    "                    ec_data[item] = [idxmap]\n",
    "\n",
    "    #计算二元频繁项\n",
    "    patterns = {}\n",
    "    for item1 in SupL1:\n",
    "        for item2 in  SupL1.keys():\n",
    "            freq = len(set(ec_data[item1])&set(ec_data[item2]))\n",
    "            if freq>=support:\n",
    "                 patterns.update({(item1,item2):freq})\n",
    "\n",
    "    print(\"二元项集数目\", len(patterns))\n",
    "    ana_time = time.time()-st\n",
    "    print(\"完成关联分析所花时间 %.3f!\" %ana_time)\n",
    "\n",
    "\n",
    "    #关联挖掘\n",
    "    pair_rules = {}\n",
    "    for pair, freq in patterns.items():\n",
    "        if len(pair) == 2:\n",
    "            cond_prob = freq / min(SupL1[pair[0]], SupL1[pair[1]])\n",
    "            if SupL1[pair[0]] > SupL1[pair[1]]:\n",
    "                pair = (pair[1], pair[0])\n",
    "            else:\n",
    "                pair = (pair[0], pair[1])\n",
    "            lift_prob = len(itemsets) * freq / (SupL1[pair[0]]*SupL1[pair[1]])\n",
    "            if lift_prob >= lift and cond_prob >= conf:\n",
    "                pair_rules[pair] = freq           \n",
    "    ###\n",
    "    info_cols = ['alarm_title', 'alarm_level', 'deveice_name', 'device_type_name', 'vendor', 'specialtyid']\n",
    "    rule_cols = ['master_title', 'master_level', 'master_device_name', 'master_device_type', 'master_vendor','master_specialty',\n",
    "                 'slave_title', 'slave_level', 'slave_device_name', 'slave_device_type', 'slave_vendor','slave_specialty',\n",
    "                 'support', 'master_freq', 'slave_freq', 'total_itemsets', 'confidence', 'lift']\n",
    "\n",
    "    total_itemsets = int(len(itemsets))\n",
    "\n",
    "    for (master, slave), freq in pair_rules.items():\n",
    "        tmp = info_table[info_table['pair_id']==master][info_cols].values.tolist()[0]\n",
    "        tmp.extend(info_table[info_table['pair_id']==slave][info_cols].values.tolist()[0])\n",
    "        tmp.extend([freq, int(SupL1[master]),\n",
    "                 int(SupL1[slave]),\n",
    "                 total_itemsets,\n",
    "                 freq/int(SupL1[master]),\n",
    "                 freq*len(itemsets)/int(SupL1[master])/int(SupL1[slave])])\n",
    "        rule.append(tmp)\n",
    "\n",
    "#     rule = pd.DataFrame(rule, columns=rule_cols)\n",
    "    print(\"完成关联挖掘所花时间%.3f！\"%(time.time()-st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将rule转换DataFrame格式\n",
    "rule = pd.DataFrame(rule, columns=rule_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10、对初步规则进行筛选和泛化，生成最终规则，保存文件名为MSNe_Rules.csv\n",
    "st = time.time()\n",
    "#保留主告警等级<=次告警等级，去除存在衍生告警的告警\n",
    "rule = rule[rule.master_level<=rule.slave_level].copy()\n",
    "rule['wrong_flag'] = rule.master_title.apply(lambda x:1 if '衍生' in x else 0)\n",
    "#主次告警的标题和网元类型若相同，black_flag=1 否则black_flag=0\n",
    "rule['black_flag'] = 0\n",
    "rule.loc[(rule.master_title==rule.slave_title)&(rule.master_device_name==rule.slave_device_name), 'black_flag'] = 1\n",
    "#拓扑关系校验，存在拓扑关系，topo_flag;否则topo_flag=0\n",
    "# def TopoTest(x):\n",
    "#       return int(x[0]==x[1] or (x[0] in G) and (x[1] in G) and (nx.has_path(G, x[0], x[1])))\n",
    "# rule['topo_flag'] = rule[['master_device_id', 'slave_device_id']].apply(TopoTest, axis=1)\n",
    "#保留拓扑与有效规则\n",
    "rule = rule.query('black_flag==0').query('wrong_flag==0').copy()\n",
    "#去除flag字段\n",
    "rule.drop(['black_flag'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "rule['same_flag'] = 0\n",
    "rule.loc[rule.master_device_name==rule.slave_device_name, 'same_flag'] = 1\n",
    "#将主次告警表存到本地路径下\n",
    "rule.to_csv('MSID_Rules.csv', sep=',', index=None)\n",
    "print(\"完成规则筛选所花时间 %3.f\" %(time.time()-st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对粗糙规则进行规则泛化，保证规则的可适应性\n",
    "gpcols = ['same_flag', 'master_title', 'master_vendor', 'master_device_type','master_specialty',\n",
    "          'slave_title', 'slave_vendor', 'slave_device_type', 'slave_specialty']\n",
    "aggfunc = {'support':'sum', 'master_freq':'sum', 'slave_freq':'sum'}\n",
    "aggthis = rule.groupby(gpcols).agg(aggfunc)\n",
    "aggthis['conf'] = aggthis['support'] / aggthis['master_freq']\n",
    "aggthis['lift'] = aggthis['support'] * len(itemsets) / ( aggthis['master_freq'] *  aggthis['slave_freq'])\n",
    "aggthis.reset_index(inplace=True)\n",
    "aggthis.to_csv('MSNe_Rules.csv', index=None, sep=',')\n",
    "print(\"完成网元类型表！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggthis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
